{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/weka/nora-default/varshak/pyenv/versions/miniconda3-3.9-24.1.2-0/envs/bright/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "from retrievers import calculate_retrieval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_examples = load_dataset('xlangai/bright', f\"examples\", cache_dir=\"cache\")[\"theoremqa_theorems\"]\n",
    "examples = {}\n",
    "for e in raw_examples:\n",
    "    examples[e['id']] = e\n",
    "\n",
    "raw_examples1 = load_dataset('xlangai/bright', f\"gpt4_reason\", cache_dir=\"cache\")[\"theoremqa_theorems\"]\n",
    "examples1 = {}\n",
    "for e in raw_examples1:\n",
    "    examples1[e['id']] = e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li1 = [ex['id'] for ex in examples.values()]\n",
    "li2 = [ex['id'] for ex in examples1.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76, 78)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(li1), len(li2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_examples = load_dataset('xlangai/bright', f\"gpt4_reason\", cache_dir=\"cache\")[\"economics\"]\n",
    "examples = {}\n",
    "for e in raw_examples:\n",
    "    examples[e['id']] = e\n",
    "\n",
    "doc_pairs = load_dataset('xlangai/bright', 'documents', cache_dir=\"cache\")[\"economics\"]\n",
    "documents = {}\n",
    "for d in doc_pairs:\n",
    "    documents[d['id']] = d['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"outputs/economics_bm25_long_False_reasoning_gpt4/score.json\") as f:\n",
    "    all_scores = json.load(f)\n",
    "new_scores = copy.deepcopy(all_scores)\n",
    "\n",
    "with open(\"r1_score_5x_0_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_0 = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"r1_score_5x_1_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_1 = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"r1_score_5x_2_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_2 = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"r1_score_5x_3_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_3 = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"r1_score_5x_4_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_4 = [json.loads(line) for line in f]\n",
    "\n",
    "with open(\"llama_score_5x_0_rank_r_results.jsonl\", \"r\") as f:\n",
    "    data_b = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 33\n",
      "15 32\n",
      "23 38\n",
      "27 32\n",
      "39 39\n",
      "43 34\n",
      "52 34\n",
      "53 41\n",
      "59 33\n",
      "61 36\n",
      "74 33\n",
      "92 31\n",
      "94 49\n",
      "95 31\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(data)):\n",
    "    # print disagreements in data and datab\n",
    "    da = ((np.asanyarray(data[i]['scores']) != np.asanyarray(data_b[i]['scores'])).sum())\n",
    "    # print(da)\n",
    "    if da>30:\n",
    "        print(i, da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 103/103 [00:00<00:00, 4612.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# extract doc order\n",
    "dids = []\n",
    "for qid,scores in tqdm(all_scores.items()):\n",
    "    docs = []\n",
    "    sorted_scores = sorted(scores.items(),key=lambda x:x[1],reverse=True)[:100]\n",
    "    for did, _ in sorted_scores:\n",
    "        docs.append([did, documents[did]])\n",
    "    dids.append([did for did, _ in sorted_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'interest_rate_parity/eurron_13.txt'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ind = 21\n",
    "# dids[ind].index(\"dollar_flow/Circularflowofincome_16.txt\")\n",
    "dids[ind][34]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (data[ind]['scores']), (data_b[ind]['scores'])\n",
    "data[ind]['scores'][72]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.2, 0.6, 0.4, 0.2, 0.4, 0.2, 0.2, 0.8, 0.4, 0.4, 0.0, 0.0, 0.4, 0.4, 0.2, 0.4, 0.2, 0.2, 0.2, 0.4, 0.2, 0.4, 0.4, 0.2, 0.2, 0.0, 0.2, 0.2, 0.0, 0.0, 0.2, 0.4, 0.4, 0.2, 0.2, 0.2, 0.4, 0.0, 0.0, 0.0, 0.4, 0.2, 0.0, 0.2, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.2, 0.0, 0.0, 0.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.2, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2, 0.4, 0.2, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.4, 0.0, 0.0, 0.4, 0.4, 0.2, 0.0, 0.0, 0.0]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(data[ind]['scores'])\n",
    "print(np.where(np.asarray(data[ind]['scores']) == 1.0)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance score: 3\n",
      "<end_answer>\n",
      "</think><think>\n",
      "Okay, so I need to determine if the provided document is relevant to the query about GDP calculation and whether it can be negative, especially in a simplified economy where a toy is produced and sold at a loss. Let me break this down step by step.\n",
      "\n",
      "First, the query is about understanding GDP calculation, specifically whether GDP can be negative. The example given involves producing a toy in one year and selling it at a loss in the next, and how that affects GDP. The query also touches on inventory changes and their impact on GDP components like investment and consumption.\n",
      "\n",
      "Looking at the document, it's from a chapter titled \"Change in Private Inventories\" and discusses how the Bureau of Economic Analysis (BEA) adjusts inventory data. It goes into detail about accounting methods, price indexes, and how inventories are valued. The document mentions the Inventory Valuation Adjustment (IVA) and how it's used to remove holding gains or losses from inventory changes. It also explains the perpetual inventory method and how real and current-dollar estimates are derived.\n",
      "\n",
      "Now, the query is concerned with how producing and selling a product at a loss affects GDP. The document explains that changes in inventory affect GDP through the investment component. When goods are produced and not sold, they contribute to inventory, which is part of investment. If they're sold later, even at a loss, that sale contributes to consumption. The document also clarifies that GDP measures the value of production, not the profit or loss from sales, so GDP remains positive.\n",
      "\n",
      "The document doesn't directly address the simplified example with the toy but does cover the relevant concepts. It explains how inventories are accounted for in GDP, which directly relates to the query's concerns about production and sales across different years. The discussion on IVA and inventory changes helps in understanding that losses from sales don't negate the initial production's contribution to GDP.\n",
      "\n",
      "However, the document is quite technical and focused on the BEA's methods, which might not directly answer the query's specific example but does provide the necessary background to understand why GDP isn't negative in such scenarios. It supports the conclusion that GDP remains positive because it's based on production value, not sale outcomes.\n",
      "\n",
      "So, while the document doesn't explicitly address the toy example, it's relevant because it explains the underlying principles of inventory changes and their impact on GDP calculation. Therefore, the relevance score should be a 3 because it provides useful but not directly explicit information for the query.\n",
      "</think>\n",
      "\n",
      "Relevance score: 3\n"
     ]
    }
   ],
   "source": [
    "print(data[ind]['outputs'][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forecasted period might be determined\n",
      "by the future evolution of several macroeconomic factors such as: the increase\n",
      "of the current account deficit correlated\n",
      "with the potential increase in the public\n",
      "budget deficit (the twin deficits), the ascending trend of the Romanian interest\n",
      "rates on the money market, the upward\n",
      "trend of inflation in Romania, or the expansionary fiscal policy which has begun\n",
      "in 2016 as well as by changing regulations\n",
      "and political instability.\n",
      "It worth’s to be mentioned that recently the National Bank of Romania [15]\n",
      "has launched the national plan for euro\n",
      "adoption in order to map the future path\n",
      "to be followed by decision makers in this\n",
      "regard, arguing that it is only a matter\n",
      "of timing for passing to euro. It is ascertained the complexity of this process and\n",
      "the careful assessment of the full accomplishment of all convergence criteria so\n",
      "as to witness a smooth euro adoption.\n",
      "SWS INTERNATIONAL\n",
      "SOCIETY\n",
      "SWS Journal of Social Sciences and Art\n",
      "- 41 - DOI 10.35603/SSA2019/ISSUE2.03\n",
      "REFERENCE\n",
      "[1] Zaharia, M. and C. Oprea. Econometric Analysis of RON/EUR Exchange Rate Evolution in the Period 2005-2011, Annals of the Oradea University. Fascicle of Management and Technological Engineering, Volume X (XX), no.1, 2011.\n",
      "[2] Petrica, A.C. and S. Stancu. Empirical Results of Modeling EUR/RON Exchange Rate\n",
      "using ARCH, GARCH, EGARCH, TARCH and PARCH models, Romanian Statistical\n",
      "Review nr. 1 / 2017\n",
      "[3] Rout, M., B. Majhi, R. Majhi and G. Panda. Forecasting of currency exchange rates\n",
      "using an adaptive ARMA model with differential evolution based training,Journal of\n",
      "King Saud University – Computer and Information Sciences, no. 26, pp. 7-18, 2014\n",
      "[4] Babu, A.S. and S.K. Reddy. Exchange Rate Forecasting using ARIMA, Neural Network and FuzzyNeuron, Journal of Stock and Forex Trading, vol.4, issue 3, 2015.\n",
      "[5] Ghalayini, L. Modeling and Forecasting the US Dollar/Euro Exchange Rate, International Journal of Economics and Finance; Vol. 6, No. 1; 2014\n",
      "[6] Maniatis. P. Forecasting the Exchange Rate between Euro and USD: Probabilistic\n",
      "Approach versus ARIMA and Exponential Smoothing Techniques, The Journal of Applied Business Research – March/April 2012, vol. 28, no.2.\n",
      "[7] Dunis, C. and X. Huang. Forecasting and trading currency volatility: An application\n",
      "of recurrent neural regression and model combination, Liverpool Business School\n",
      "Working Paper, 2002.\n",
      "[8] Gujarati, D.N. Basic econometrics, 4th edition, McGraw-Hill Company, 2004.\n",
      "[9] Greene, W.H. Econometric Analysis, 5th edition, Pearson Education, Inc., Upper Saddle River,New Jersey, 2002\n",
      "[10] Stellwagen, E. ARIMA forecasting: Conceptual Overview, 2017, available at:\n",
      "http://www.forecastpro.com/Trends/forecasting101June2012.html\n"
     ]
    }
   ],
   "source": [
    "print(documents[\"interest_rate_parity/eurron_13.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for combing with BM25\n",
    "for qid,scores in tqdm(all_scores.items()):\n",
    "    for i, output in enumerate(data[int(qid)]['outputs']):\n",
    "        data[int(qid)]['ranking'][dids[int(qid)][i]] = data[int(qid)]['ranking'][dids[int(qid)][i]]*100 + scores[dids[int(qid)][i]]\n",
    "\n",
    "for qid,scores in tqdm(all_scores.items()):\n",
    "    new_scores[qid] = data[int(qid)]['ranking']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([34])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_5x = []\n",
    "for i in range(5):\n",
    "    with open(f\"r1_score_5x_{i}_rank_r_results.jsonl\", \"r\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "        scores_5x.append(data[21]['scores'])\n",
    "scores_5x = np.asarray(scores_5x)\n",
    "np.where(np.any(scores_5x > 0.6, axis=0) & np.any(scores_5x < 0.4, axis=0))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0224, 0.0064, 0.032 , 0.0096, 0.0096, 0.0224, 0.0096, 0.    ,\n",
       "       0.0064, 0.    , 0.0096, 0.0064, 0.0064, 0.0064, 0.0096, 0.0096,\n",
       "       0.    , 0.0096, 0.0224, 0.0064, 0.    , 0.0096, 0.    , 0.0064,\n",
       "       0.    , 0.0064, 0.    , 0.    , 0.0096, 0.    , 0.    , 0.0064,\n",
       "       0.    , 0.    , 0.    , 0.0096, 0.0096, 0.    , 0.    , 0.0096,\n",
       "       0.0096, 0.0064, 0.    , 0.    , 0.    , 0.    , 0.    , 0.0064,\n",
       "       0.    , 0.    , 0.0064, 0.    , 0.    , 0.    , 0.0064, 0.    ,\n",
       "       0.0064, 0.0064, 0.0064, 0.    , 0.    , 0.0096, 0.    , 0.0384,\n",
       "       0.    , 0.0064, 0.016 , 0.    , 0.0064, 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.    , 0.    , 0.    , 0.0256, 0.    , 0.0096, 0.    ,\n",
       "       0.0096, 0.    , 0.    , 0.    , 0.0096, 0.0096, 0.    , 0.0064,\n",
       "       0.    , 0.    , 0.0096, 0.    , 0.    , 0.    , 0.    , 0.    ,\n",
       "       0.    , 0.0096, 0.    , 0.    ])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_5x = np.asarray(scores_5x)\n",
    "# find the column with the highest variance\n",
    "np.var(scores_5x, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0. , 0.8, 0.2, 0.2, 0. ])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_5x[:, 34] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I want to find a column that has a value >0.5 and <0.5\n",
    "np.where((scores_5x[:, 1] > 0.5) & (scores_5x[:, 1] < 0.6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to figure out if the provided document is relevant to the query. The query is about why there aren't many studies on forecasting exchange rates of Central Bank Digital Currencies (CBDCs) compared to cryptocurrencies. It goes into details about the nature of CBDCs, their pegged exchange rates, and reasons why research might be lacking.\n",
      "\n",
      "Looking at the document, it's about forecasting the RON/EUR exchange rate. It discusses macroeconomic factors like current account deficits, interest rates, inflation, and fiscal policies. It also mentions the National Bank of Romania's plan to adopt the euro and references several studies on exchange rate modeling using various econometric models like ARIMA, GARCH, etc.\n",
      "\n",
      "So, the document is focused on traditional fiat currency exchange rates, specifically RON to EUR. It doesn't mention CBDCs at all. The query is about the lack of studies on CBDC exchange rate forecasting. Since CBDCs are pegged to their fiat currencies, their exchange rates mirror the fiat ones, making specific studies on CBDCs redundant. The document supports this by showing that existing models for fiat currencies (like RON/EUR) are well-researched, which aligns with the query's points about redundancy and why CBDC-specific studies aren't needed.\n",
      "\n",
      "Therefore, the document is relevant because it exemplifies the kind of research that already exists for fiat currencies, indirectly explaining why similar studies for CBDCs might be lacking. It provides evidence for one of the reasons discussed in the query.\n",
      "</think>\n",
      "\n",
      "The document discusses forecasting traditional fiat currency exchange rates, which relates to the query's point about redundancy in CBDC studies. It supports the idea that existing models cover CBDCs indirectly, making specific CBDC studies less necessary.\n",
      "\n",
      "Relevance score: 4.\n"
     ]
    }
   ],
   "source": [
    "print(data_1[ind]['outputs'][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, so I need to determine if the provided document is relevant to the query. The query is about why there's a lack of studies on forecasting exchange rates for Central Bank Digital Currencies (CBDCs), especially compared to cryptocurrencies. The document, on the other hand, seems to discuss exchange rate forecasting models, specifically for the RON/EUR rate, using econometric models like ARIMA, GARCH, etc.\n",
      "\n",
      "First, the query is focused on CBDCs and why their exchange rate forecasting isn't studied as much. The document doesn't mention CBDCs at all. It talks about traditional fiat currencies, like the Romanian Lei (RON) and the Euro (EUR). So, the subject matter is different.\n",
      "\n",
      "Next, the query explores reasons such as the pegged nature of CBDCs, their developmental stage, and regulatory aspects. The document, however, is about applying various forecasting models to existing fiat currencies. It doesn't touch on digital currencies, let alone CBDCs.\n",
      "\n",
      "The query also mentions that CBDC exchange rates mirror their fiat counterparts, making specific studies redundant. The document supports this indirectly by showing how traditional models are used for fiat currencies, which would apply to CBDCs. But the document itself doesn't discuss CBDCs or their implications.\n",
      "\n",
      "So, while the document is about exchange rate forecasting, it's focused on traditional currencies and doesn't address the specific query about CBDCs. Therefore, it's not directly relevant to the question posed.\n",
      "</think>\n",
      "\n",
      "The document discusses exchange rate forecasting for traditional fiat currencies, specifically RON/EUR, using econometric models. It does not mention CBDCs or address the reasons for the lack of studies on their exchange rates. \n",
      "\n",
      "Relevance score: 1.\n"
     ]
    }
   ],
   "source": [
    "print(data_2[ind]['outputs'][34])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bright",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
